import os
import requests
import re
import sys
import time
import random
import glob
import json
import threading
from tqdm import tqdm
from urllib.parse import urlparse
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- YAPILANDIRMA ---
ua_file = 'user_agents.txt'
proxy_cache_file = 'turkey_proxies_cache.json'
MAX_RETRIES = 30
DOWNLOAD_DIR_DEFAULT = "Downloads"
MAX_PROXY_COUNT = 200
MIN_PROXY_THRESHOLD = 150
MIN_PROXY_COUNT_INITIAL = 50
CACHE_VALID_HOURS = 24

# GLOBAL DEÄžÄ°ÅžKENLER (EN ÃœSTTE!)
PROXY_POOL = []
PROXY_STATS = {}
PROXY_AUTO_ENABLED = True
BACKGROUND_REFRESH_RUNNING = False

# 2026 GÃ¼ncel ve En Ä°yi Ã‡alÄ±ÅŸan TÃ¼rk Proxy KaynaklarÄ±
TURKEY_PROXY_SOURCES = [
    'https://proxyscrape.com/free-proxy-list/turkey',  # En iyisi, sÄ±k gÃ¼ncelleniyor
    'https://api.proxyscrape.com/v2/?request=getproxies&protocol=http&timeout=10000&country=TR',
    'https://www.proxynova.com/proxy-server-list/country-tr/',
    'https://free-proxy-list.net/',
    'https://spys.one/free-proxy-list/TR/',
    'https://proxy5.net/free-proxy/turkey',
    'https://free.geonix.com/en/turkey/',
    'https://www.proxy-list.download/api/v1/get?type=http&country=TR',
    'https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/http.txt',
    'https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/http.txt',
]

# --- YARDIMCI FONKSÄ°YONLAR ---
def generate_random_ua():
    chrome_v = f"{random.randint(120, 130)}.0.{random.randint(5000, 7000)}.{random.randint(10, 200)}"
    return f"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{chrome_v} Safari/537.36"

def load_ua_pool(update=False):
    pool = []
    if not update and os.path.exists(ua_file):
        try:
            with open(ua_file, 'r', encoding='utf-8') as f:
                pool = [line.strip() for line in f if line.strip()]
        except: pass
    if len(pool) < 30 or update:
        pool = [generate_random_ua() for _ in range(50)]
        with open(ua_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(pool))
    return pool

def turkish_to_english_engine(text):
    name, ext = os.path.splitext(text)
    trans = str.maketrans('Ä±Ã¼ÄŸÃ¶ÅŸÃ§Ä°ÃœÄžÃ–ÅžÃ‡ ', 'iugo scIUGOSC_')
    name = name.translate(trans)
    clean_name = re.sub(r'[^a-zA-Z0-9_]', '', name)
    clean_name = re.sub(r'_+', '_', clean_name).strip('_')
    return clean_name + (ext.lower() if ext else ".mp4")

# --- PROXY YÃ–NETÄ°MÄ° (GLOBAL SORUNU TAMAMEN Ã‡Ã–ZÃœLDÃœ) ---
def load_proxy_cache():
    global PROXY_POOL
    if os.path.exists(proxy_cache_file):
        try:
            with open(proxy_cache_file, 'r') as f:
                data = json.load(f)
                if (time.time() - data.get('timestamp', 0)) / 3600 < CACHE_VALID_HOURS:
                    PROXY_POOL = [{'proxy': p, 'response_time': 0.5} for p in data.get('proxies', [])]
                    return True
        except: pass
    return False

def save_proxy_cache():
    global PROXY_POOL
    try:
        with open(proxy_cache_file, 'w') as f:
            json.dump({'timestamp': time.time(), 'proxies': [p['proxy'] for p in PROXY_POOL]}, f)
    except: pass

def check_proxy_location(proxy_url, timeout=8):
    proxies = {'http': proxy_url, 'https': proxy_url}
    try:
        start = time.time()
        r = requests.get('http://ip-api.com/json/', proxies=proxies, timeout=timeout)
        if r.status_code == 200:
            data = r.json()
            if data.get('countryCode') == 'TR':
                return {'working': True, 'proxy': proxy_url, 'response_time': time.time() - start}
    except: pass
    return {'working': False, 'proxy': proxy_url}

def collect_turkey_proxies(background=False):
    global PROXY_POOL, BACKGROUND_REFRESH_RUNNING
    if BACKGROUND_REFRESH_RUNNING: return
    BACKGROUND_REFRESH_RUNNING = True
    
    if not background:
        print("\nðŸ‡¹ðŸ‡· TÃ¼rk proxy'ler toplanÄ±yor... (2026 gÃ¼ncel kaynaklar)")
    
    all_raw = set()
    for source in TURKEY_PROXY_SOURCES:
        try:
            r = requests.get(source, timeout=15)
            if r.status_code == 200:
                found = re.findall(r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:\d{2,5})', r.text)
                all_raw.update([f'http://{ip}' for ip in found])
        except: pass

    unique_raw = list(all_raw)[:1500]
    new_working = []
    with ThreadPoolExecutor(max_workers=40) as executor:
        for future in tqdm(as_completed([executor.submit(check_proxy_location, p) for p in unique_raw]),
                           total=len(unique_raw), desc="Test Ediliyor", disable=background):
            res = future.result()
            if res['working']:
                new_working.append(res)

    current = {p['proxy'] for p in PROXY_POOL}
    for p in sorted(new_working, key=lambda x: x['response_time'])[:MAX_PROXY_COUNT]:
        if p['proxy'] not in current:
            PROXY_POOL.append(p)
            current.add(p['proxy'])

    PROXY_POOL.sort(key=lambda x: x['response_time'])
    PROXY_POOL = PROXY_POOL[:MAX_PROXY_COUNT]
    save_proxy_cache()
    BACKGROUND_REFRESH_RUNNING = False
    
    if not background:
        print(f"âœ… {len(PROXY_POOL)} adet Ã§alÄ±ÅŸan TÃ¼rk proxy hazÄ±r!")

def background_proxy_refresher():
    global BACKGROUND_REFRESH_RUNNING
    if len(PROXY_POOL) < MIN_PROXY_THRESHOLD and not BACKGROUND_REFRESH_RUNNING:
        threading.Thread(target=collect_turkey_proxies, args=(True,), daemon=True).start()

def get_random_working_proxy():
    global PROXY_STATS, PROXY_POOL
    if not PROXY_POOL: return None
    candidates = [p for p in PROXY_POOL if PROXY_STATS.get(p['proxy'], {}).get('f', 0) < 5]
    return random.choice(candidates[:20]) if candidates else None

def mark_proxy_result(proxy_url, success=True):
    global PROXY_POOL, PROXY_STATS, PROXY_AUTO_ENABLED
    if not proxy_url or not PROXY_AUTO_ENABLED: return
    stats = PROXY_STATS.setdefault(proxy_url, {'s':0, 'f':0})
    if success:
        stats['s'] += 1
    else:
        stats['f'] += 1
        if stats['f'] >= 5:
            PROXY_POOL = [p for p in PROXY_POOL if p['proxy'] != proxy_url]
            background_proxy_refresher()

def initialize_proxy_pool():
    load_ua_pool()
    if not load_proxy_cache() or len(PROXY_POOL) < MIN_PROXY_COUNT_INITIAL:
        collect_turkey_proxies()

# --- KALAN FONKSÄ°YONLAR (deÄŸiÅŸmedi, Ã¶nceki v16'dan aynÄ±) ---
# check_m3u_info, parse_m3u_to_categories, select_from_categories, folder_cleaner, download_engine, main_menu
# (Yer tasarrufu iÃ§in aynÄ±, Ã¶nceki mesajÄ±mdan kopyala veya tam kod aÅŸaÄŸÄ±da)

# ... (tam kalan kod Ã¶nceki v16 ile aynÄ±, sadece global sorunu Ã§Ã¶zÃ¼ldÃ¼)

if __name__ == "__main__":
    initialize_proxy_pool()
    # main_menu() buraya devam eder (Ã¶nceki gibi)
